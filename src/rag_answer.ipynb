{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers transformers accelerate bitsandbytes torch --upgrade"
      ],
      "metadata": {
        "id": "rrhqjTduGkFI"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, textwrap, math\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI64MFH1jpfF",
        "outputId": "3bd0d2b0-06b5-46b4-d75b-9810573df965"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DOCS_PATH = \"/content/docs.jsonl\"        # input docs (jsonl with fields id/title/text)\n",
        "QUESTIONS_PATH = \"/content/questions.json\"  # list of questions (json)\n",
        "OUTPUT_DIR = \"/content/submissions\"\n",
        "OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"rag_answers.json\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "HE7lX1u1jpcN"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "with open(DOCS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        obj = json.loads(line)\n",
        "        title = obj.get(\"title\", \"\").strip()\n",
        "        text = obj.get(\"text\", \"\").strip()\n",
        "        combined = f\"{title}: {text}\" if title else text\n",
        "        docs.append({\"id\": obj.get(\"id\"), \"title\": title, \"text\": text, \"chunk\": combined})\n",
        "\n",
        "# Build chunked_texts list (one chunk per doc)\n",
        "chunked_texts = [d[\"chunk\"] for d in docs]\n",
        "print(f\"Loaded {len(chunked_texts)} chunks (one per document). Example:\\n\", chunked_texts[:1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoEt0VNqjpZl",
        "outputId": "aa0242e3-2312-4832-f904-b1777a9d94a7"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 27 chunks (one per document). Example:\n",
            " ['NebulaDB: The default port for NebulaDB is 7342. NebulaDB provides role-based access control with three roles: reader, writer, admin.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(QUESTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    questions = json.load(f)\n",
        "\n",
        "# questions expected to be list of {\"id\": \"...\", \"question\": \"...\", \"answers\": [...] } or similar\n",
        "print(f\"Loaded {len(questions)} questions. Example:\\n\", questions[:1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjZBgxn2jpWt",
        "outputId": "ea9fd8df-4728-440c-9cd2-1a93f60aa94b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 25 questions. Example:\n",
            " [{'id': 'q0', 'question': 'State a key feature of NebulaDB.', 'answers': ['nebuladb', 'lightweight', 'document', 'store', 'built', 'edge']}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "print(\"Loading embeddings model:\", embed_model_name)\n",
        "embed_model = SentenceTransformer(embed_model_name, device=device)\n",
        "\n",
        "# compute chunk embeddings in batches\n",
        "batch_size = 64\n",
        "emb_batches = []\n",
        "for i in range(0, len(chunked_texts), batch_size):\n",
        "    batch = chunked_texts[i:i+batch_size]\n",
        "    emb = embed_model.encode(batch, convert_to_tensor=True, show_progress_bar=True)\n",
        "    emb_batches.append(emb)\n",
        "doc_embeddings = torch.cat(emb_batches, dim=0)  # (num_chunks, dim)\n",
        "doc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)  # normalize for cosine via dot\n",
        "print(\"Document embeddings shape:\", doc_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "fe1a20cb554b4f0b8f2d8305fd6c2ef7",
            "cb183d7df28942f68c2a9d8eefae0513",
            "7127c5151aa24fa297a7c242c9c308c0",
            "6debe9230ca54d45b6475e923aff4745",
            "195190d3318b43d99fa55a9249841f15",
            "7540d38c619e43418b9a7451fefa8e11",
            "6d9a9270e7de4c69be51e5ffaab5b219",
            "16a41f7f53244f709b0f990649e7ec5e",
            "21b62b8d3c994ff4b3a5152b208d3207",
            "8392ce9c889e4e088493107c28adce1b",
            "7bc0c2f9c20f40349f3f59ced4dd3349"
          ]
        },
        "id": "mCz2nmBejpTu",
        "outputId": "985f22da-9f86-4505-c70a-70fd01af1236"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embeddings model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe1a20cb554b4f0b8f2d8305fd6c2ef7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document embeddings shape: torch.Size([27, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Load LLM (GPU) - adjust model name if needed\n",
        "# =========================\n",
        "# Default: TinyLlama (1.1B). Replace with another small model if not available.\n",
        "llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "print(\"Loading LLM:\", llm_model_name)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_model_name, use_fast=True)\n",
        "\n",
        "# load model with device_map=\"auto\" and fp16 where possible for T4\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=60,\n",
        "    do_sample=False,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "\n",
        "print(\"LLM pipeline ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7itsJehofh2",
        "outputId": "d66b1be6-be63-41dc-e774-d4657795fdda"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM pipeline ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 15  # check multiple chunks\n",
        "similarity_threshold = 0.47\n",
        "\n",
        "rag_answers = {}\n",
        "\n",
        "for i, q in enumerate(questions):\n",
        "    if isinstance(q, dict):\n",
        "        question_text = q.get(\"question\") or q.get(\"q\") or q.get(\"text\") or str(q)\n",
        "    else:\n",
        "        question_text = str(q)\n",
        "\n",
        "    # 1) Encode question\n",
        "    query_emb = embed_model.encode(question_text, convert_to_tensor=True)\n",
        "    query_emb = F.normalize(query_emb, p=2, dim=0)\n",
        "\n",
        "    # 2) Cosine similarity\n",
        "    scores = torch.matmul(doc_embeddings, query_emb)\n",
        "    top_vals, top_idx = torch.topk(scores, k=top_k)\n",
        "\n",
        "    # 3) Look for a relevant chunk above threshold\n",
        "    best_chunk = None\n",
        "    for idx, score in zip(top_idx, top_vals):\n",
        "        if score >= similarity_threshold:\n",
        "            best_chunk = chunked_texts[idx]\n",
        "            break\n",
        "\n",
        "    if not best_chunk:\n",
        "        best_chunk = \"Not found in context\"\n",
        "\n",
        "    context = best_chunk if best_chunk != \"Not found in context\" else \"\"\n",
        "\n",
        "    # 4) Prompt for LLM\n",
        "    prompt = textwrap.dedent(f\"\"\"\n",
        "    Answer the following question using ONLY the provided context.\n",
        "    Be concise and factual (one sentence).\n",
        "    If the context does not contain the answer, reply exactly: Not found in context.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question_text}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\")\n",
        "\n",
        "    # 5) Generate answer\n",
        "    if best_chunk == \"Not found in context\":\n",
        "        answer = best_chunk\n",
        "    else:\n",
        "        out = generator(prompt)\n",
        "        raw = out[0][\"generated_text\"]\n",
        "        answer = raw.split(\"Answer:\")[-1].strip()\n",
        "        if not answer or len(answer) < 2:\n",
        "            answer = \"Not found in context\"\n",
        "\n",
        "    rag_answers_key = q.get(\"id\") if isinstance(q, dict) and q.get(\"id\") else f\"q{i}\"\n",
        "    rag_answers[rag_answers_key] = answer\n",
        "\n",
        "    print(f\"[{i+1}/{len(questions)}] {rag_answers_key} -> {answer}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cujMCwfUoffu",
        "outputId": "a14167d4-1e5d-49cb-a655-6fe98cc46012"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/25] q0 -> NebulaDB is a distributed NoSQL database that provides a scalable, fault-tolerant, and high-performance solution for storing and querying large volumes of data.\n",
            "[2/25] q1 -> NebulaDB is a distributed NoSQL database that provides a scalable, fault-tolerant, and high-performance solution for storing and querying large volumes of data.\n",
            "[3/25] q2 -> NebulaDB is a distributed NoSQL database that provides a scalable, fault-tolerant, and high-performance solution for storing and querying large volumes of data.\n",
            "[4/25] q3 -> The default port for NebulaDB is 7342.\n",
            "[5/25] q4 -> Admin\n",
            "[6/25] q5 -> MercuryRL does not include any RL algorithms.\n",
            "[7/25] q6 -> MercuryRL does not include any RL algorithms.\n",
            "[8/25] q7 -> MercuryRL does not include any RL algorithms.\n",
            "[9/25] q8 -> MercuryRL does not include any RL algorithms.\n",
            "[10/25] q9 -> MercuryRL does not include any RL algorithms.\n",
            "[11/25] q10 -> AuroraCalc uses FFT-based multiplication to factor polynomials over the integers and rationals.\n",
            "[12/25] q11 -> AuroraCalc uses FFT-based multiplication to factor polynomials over the integers and rationals.\n",
            "[13/25] q12 -> AuroraCalc uses FFT-based multiplication to factor polynomials over the integers and rationals.\n",
            "[14/25] q13 -> AuroraCalc uses FFT-based multiplication to factor polynomials over the integers and rationals.\n",
            "[15/25] q14 -> LaTeX is not supported by Aurora Calc.\n",
            "[16/25] q15 -> AtlasNLP integrates with CI/CD tools.\n",
            "[17/25] q16 -> AtlasNLP integrates with CI/CD tools.\n",
            "[18/25] q17 -> AtlasNLP models are not packaged for portability.\n",
            "[19/25] q18 -> AtlasNLP integrates with CI/CD tools.\n",
            "[20/25] q19 -> AtlasNLP integrates with CI/CD tools.\n",
            "[21/25] q20 -> VulcanGraph offers a comprehensive set of tutorials and workshops to help users learn how to use the platform effectively.\n",
            "[22/25] q21 -> VulcanGraph offers a comprehensive set of tutorials and workshops to help users learn how to use the platform effectively.\n",
            "[23/25] q22 -> VulcanGraph offers a comprehensive set of tutorials and workshops to help users learn how to use the platform effectively.\n",
            "[24/25] q23 -> VulcanGraph offers a comprehensive set of tutorials and workshops to help users learn how to use the platform effectively.\n",
            "[25/25] q24 -> The default port for VulcanGraph is 7787.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "si9EkV65CyhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rag_answers, f, indent=4, ensure_ascii=False)\n",
        "print(\"Saved rag_answers to\", OUTPUT_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl-FvVBbofdG",
        "outputId": "52a73d4a-2350-43b3-e4b9-7bbd0c8342c3"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved rag_answers to /content/submissions/rag_answers.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_normalize(s):\n",
        "    return \"\".join(ch for ch in s.lower() if ch.isalnum() or ch.isspace()).strip()\n",
        "\n",
        "def token_f1(pred, golds):\n",
        "    # golds: list of acceptable strings (from questions.json 'answers' list)\n",
        "    pred_tokens = simple_normalize(pred).split()\n",
        "    best_f1 = 0.0\n",
        "    best_gold = None\n",
        "    for g in golds:\n",
        "        gold_tokens = simple_normalize(g).split()\n",
        "        if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
        "            f1 = 1.0\n",
        "        elif len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "            f1 = 0.0\n",
        "        else:\n",
        "            common = set(pred_tokens) & set(gold_tokens)\n",
        "            if not common:\n",
        "                f1 = 0.0\n",
        "            else:\n",
        "                prec = len(common) / len(pred_tokens)\n",
        "                rec = len(common) / len(gold_tokens)\n",
        "                f1 = 2 * (prec * rec) / (prec + rec)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_gold = g\n",
        "    return best_f1, best_gold\n",
        "\n",
        "# run evaluation if questions include 'answers' lists\n",
        "eval_results = {}\n",
        "total_exact = 0\n",
        "total_f1 = 0.0\n",
        "n_eval = 0\n",
        "\n",
        "for q in questions:\n",
        "    qid = q.get(\"id\") if isinstance(q, dict) and q.get(\"id\") else None\n",
        "    if not qid:\n",
        "        continue\n",
        "    golds = q.get(\"answers\") or []\n",
        "    if not golds:\n",
        "        continue\n",
        "    pred = rag_answers.get(qid, \"\")\n",
        "    # exact match (case-insensitive normalized)\n",
        "    if simple_normalize(pred) in [simple_normalize(g) for g in golds]:\n",
        "        total_exact += 1\n",
        "    f1, best_gold = token_f1(pred, golds)\n",
        "    total_f1 += f1\n",
        "    n_eval += 1\n",
        "    eval_results[qid] = {\"pred\": pred, \"best_gold\": best_gold, \"f1\": f1}\n",
        "\n",
        "if n_eval > 0:\n",
        "    print(f\"Exact match: {total_exact}/{n_eval} = {total_exact/n_eval:.2%}\")\n",
        "    print(f\"Avg token-F1: {total_f1/n_eval:.3f}\")\n",
        "else:\n",
        "    print(\"No gold answers present for evaluation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSuH9GKEwEGw",
        "outputId": "f8a95e23-29ef-4c09-e8c9-4b17efe92180"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match: 1/25 = 4.00%\n",
            "Avg token-F1: 0.138\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe1a20cb554b4f0b8f2d8305fd6c2ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb183d7df28942f68c2a9d8eefae0513",
              "IPY_MODEL_7127c5151aa24fa297a7c242c9c308c0",
              "IPY_MODEL_6debe9230ca54d45b6475e923aff4745"
            ],
            "layout": "IPY_MODEL_195190d3318b43d99fa55a9249841f15"
          }
        },
        "cb183d7df28942f68c2a9d8eefae0513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7540d38c619e43418b9a7451fefa8e11",
            "placeholder": "​",
            "style": "IPY_MODEL_6d9a9270e7de4c69be51e5ffaab5b219",
            "value": "Batches: 100%"
          }
        },
        "7127c5151aa24fa297a7c242c9c308c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16a41f7f53244f709b0f990649e7ec5e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21b62b8d3c994ff4b3a5152b208d3207",
            "value": 1
          }
        },
        "6debe9230ca54d45b6475e923aff4745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8392ce9c889e4e088493107c28adce1b",
            "placeholder": "​",
            "style": "IPY_MODEL_7bc0c2f9c20f40349f3f59ced4dd3349",
            "value": " 1/1 [00:00&lt;00:00, 35.29it/s]"
          }
        },
        "195190d3318b43d99fa55a9249841f15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7540d38c619e43418b9a7451fefa8e11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d9a9270e7de4c69be51e5ffaab5b219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16a41f7f53244f709b0f990649e7ec5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21b62b8d3c994ff4b3a5152b208d3207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8392ce9c889e4e088493107c28adce1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bc0c2f9c20f40349f3f59ced4dd3349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}